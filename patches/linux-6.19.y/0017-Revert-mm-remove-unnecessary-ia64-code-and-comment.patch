From f0b63e75fc297b939c89c2f1ffe86239ebb2b047 Mon Sep 17 00:00:00 2001
From: Tomas Glozar <tglozar@gmail.com>
Date: Thu, 11 Jan 2024 14:21:54 +0100
Subject: [PATCH 17/96] Revert "mm: remove unnecessary ia64 code and comment"

This reverts commit e99fb98d478a0480d50e334df21bef12fb74e17f.
---
 mm/Kconfig      |  2 +-
 mm/mm_init.c    | 42 +++++++++++++++++++++++++-----------------
 mm/page_owner.c |  8 ++++++++
 3 files changed, 34 insertions(+), 18 deletions(-)

diff --git a/mm/Kconfig b/mm/Kconfig
index a992f2203eb9..7ff819675763 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -723,7 +723,7 @@ config DEFAULT_MMAP_MIN_ADDR
 	  from userspace allocation.  Keeping a user from writing to low pages
 	  can help reduce the impact of kernel NULL pointer bugs.
 
-	  For most arm64, ppc64 and x86 users with lots of address space
+	  For most arm64, ia64, ppc64 and x86 users with lots of address space
 	  a value of 65536 is reasonable and should cause no problems.
 	  On arm and other archs it should not be higher than 32768.
 	  Programs which use vm86 functionality or have some need to map
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 2a809cd8e7fa..6591d738a2c9 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -1524,7 +1524,8 @@ void __init set_pageblock_order(void)
 
 	/*
 	 * Assume the largest contiguous order of interest is a huge page.
-	 * This value may be variable depending on boot parameters on powerpc.
+	 * This value may be variable depending on boot parameters on IA64 and
+	 * powerpc.
 	 */
 	pageblock_order = order;
 }
@@ -1646,8 +1647,9 @@ void __init *memmap_alloc(phys_addr_t size, phys_addr_t align,
 #ifdef CONFIG_FLATMEM
 static void __init alloc_node_mem_map(struct pglist_data *pgdat)
 {
-	unsigned long start, offset, size, end;
-	struct page *map;
+	unsigned long __maybe_unused start = 0;
+	unsigned long __maybe_unused offset = 0;
+	unsigned long end;
 
 	/* Skip empty nodes */
 	if (!pgdat->node_spanned_pages)
@@ -1655,20 +1657,26 @@ static void __init alloc_node_mem_map(struct pglist_data *pgdat)
 
 	start = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);
 	offset = pgdat->node_start_pfn - start;
-	/*
-	 * The zone's endpoints aren't required to be MAX_PAGE_ORDER
-	 * aligned but the node_mem_map endpoints must be in order
-	 * for the buddy allocator to function correctly.
-	 */
-	end = ALIGN(pgdat_end_pfn(pgdat), MAX_ORDER_NR_PAGES);
-	size =  (end - start) * sizeof(struct page);
-	map = memmap_alloc(size, SMP_CACHE_BYTES, MEMBLOCK_LOW_LIMIT,
-			   pgdat->node_id, false);
-	if (!map)
-		panic("Failed to allocate %ld bytes for node %d memory map\n",
-		      size, pgdat->node_id);
-	pgdat->node_mem_map = map + offset;
-	memmap_boot_pages_add(DIV_ROUND_UP(size, PAGE_SIZE));
+	/* ia64 gets its own node_mem_map, before this, without bootmem */
+	if (!pgdat->node_mem_map) {
+		unsigned long size;
+		struct page *map;
+
+		/*
+		 * The zone's endpoints aren't required to be MAX_PAGE_ORDER
+		 * aligned but the node_mem_map endpoints must be in order
+		 * for the buddy allocator to function correctly.
+		 */
+		end = ALIGN(pgdat_end_pfn(pgdat), MAX_ORDER_NR_PAGES);
+		size =  (end - start) * sizeof(struct page);
+		map = memmap_alloc(size, SMP_CACHE_BYTES, MEMBLOCK_LOW_LIMIT,
+				   pgdat->node_id, false);
+		if (!map)
+			panic("Failed to allocate %ld bytes for node %d memory map\n",
+			      size, pgdat->node_id);
+		pgdat->node_mem_map = map + offset;
+		memmap_boot_pages_add(DIV_ROUND_UP(size, PAGE_SIZE));
+	}
 	pr_debug("%s: node %d, pgdat %08lx, node_mem_map %08lx\n",
 		 __func__, pgdat->node_id, (unsigned long)pgdat,
 		 (unsigned long)pgdat->node_mem_map);
diff --git a/mm/page_owner.c b/mm/page_owner.c
index b3260f0c17ba..89c4b2f42745 100644
--- a/mm/page_owner.c
+++ b/mm/page_owner.c
@@ -158,6 +158,14 @@ static noinline depot_stack_handle_t save_stack(gfp_t flags)
 	depot_stack_handle_t handle;
 	unsigned int nr_entries;
 
+	/*
+	 * Avoid recursion.
+	 *
+	 * Sometimes page metadata allocation tracking requires more
+	 * memory to be allocated:
+	 * - when new stack trace is saved to stack depot
+	 * - when backtrace itself is calculated (ia64)
+	 */
 	if (current->in_page_owner)
 		return dummy_handle;
 
-- 
2.25.1

